{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70876e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from Miniproject_2.model import Module\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import empty , cat , arange\n",
    "from torch.nn.functional import fold, unfold\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.set_grad_enabled(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e546689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b44e2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(input, weight, stride=1, padding=0, dilation=1):\n",
    "    N, _, h_in, w_in = input.shape\n",
    "    out_channels, in_channels, kernel_size = weight.shape[:-1]\n",
    "    \n",
    "    assert input.shape[1] == in_channels\n",
    "    \n",
    "    h_out = int((h_in + 2*padding - dilation*(kernel_size-1)-1)/stride+1)\n",
    "    w_out = int((w_in + 2*padding - dilation*(kernel_size-1)-1)/stride+1)\n",
    "\n",
    "    x = unfold(input, kernel_size=kernel_size, padding=padding, dilation=dilation, stride=stride)\n",
    "    cΠks, L = x.shape[1], x.shape[2]\n",
    "    \n",
    "    x = torch.transpose(x, 1, 2).reshape(-1, cΠks)\n",
    "    weight_flat = weight.reshape(out_channels, cΠks)\n",
    "    \n",
    "    x = x @ weight_flat.t()\n",
    "    x = x.reshape(N, L, out_channels).transpose_(1, 2)\n",
    "    x = fold(x, output_size=[h_out, w_out], kernel_size=1, padding=0, dilation=dilation, stride=1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_transpose2d(input, weight, stride=1, padding=0, dilation=1):\n",
    "    N, _, h_in, w_in = input.shape\n",
    "    in_channels, out_channels, kernel_size = weight.shape[:-1]\n",
    "    \n",
    "    eff_input = augment(input, nzeros=stride-1, padding=kernel_size-1-padding)\n",
    "    return  conv2d(eff_input, weight.flip(2,3).transpose(0,1), stride=1, padding=0, dilation=1) \n",
    "\n",
    "\n",
    "def augment(input, nzeros, padding=0):\n",
    "    shape = input.shape\n",
    "    nold  = shape[-1]\n",
    "    nnew  = nold + (nold-1)*nzeros\n",
    "    \n",
    "    new = torch.zeros(*shape[:2], nnew, nnew)\n",
    "    new[:,:,::(nzeros+1),::(nzeros+1)] = input\n",
    "                \n",
    "    if padding: new = unfold(new,1, padding=padding).reshape(*new.shape[:2],*[new.shape[-1]+2*padding]*2)\n",
    "    return new\n",
    "\n",
    "\n",
    "def conv_backward(input, dL_dy, weight, stride=1, padding=0, dilation=1):\n",
    "    out_channels, in_channels, kernel_size = weight.shape[:-1]\n",
    "    dL_dx = conv_transpose2d(dL_dy, weight, stride=stride, padding=padding, dilation=dilation)\n",
    "\n",
    "    ignored = int(input.shape[-1]-dL_dx.shape[-1])\n",
    "    if ignored:\n",
    "        dL_dx = unfold(dL_dx, 1, padding=ignored).reshape(*dL_dx.shape[:2],*[dL_dx.shape[-1]+2*ignored]*2)\n",
    "        dL_dx = dL_dx[:,:,ignored:, ignored:]\n",
    "\n",
    "\n",
    "    dL_df = torch.zeros_like(weight.transpose(0,1))\n",
    "    dL_dy_aug = augment(dL_dy, nzeros=stride-1, padding=0)\n",
    "\n",
    "    x = input if not ignored else input[:,:,:-ignored, :-ignored]\n",
    "    for mu in range(x.shape[0]):\n",
    "        for alpha in range(in_channels):\n",
    "            dLdy = dL_dy_aug[mu].view(1, out_channels,*dL_dy_aug.shape[2:]).transpose(0,1)\n",
    "            xx   = x[mu,alpha].view(1,1,*x.shape[2:])\n",
    "            dL_df[alpha] += conv2d(xx, dLdy)[0]\n",
    "\n",
    "    dL_df.transpose_(0,1)\n",
    "    return dL_dx, dL_df\n",
    "\n",
    "\n",
    "class Conv2d():\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1):\n",
    "        self.in_channels  = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size  = kernel_size\n",
    "        self.stride   = stride\n",
    "        self.padding  = padding\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.weight   = torch.Tensor(out_channels, in_channels, kernel_size, kernel_size)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return conv2d(input, self.weight, stride=self.stride, padding=self.padding, dilation=self.dilation)\n",
    "    \n",
    "    __call__ = forward\n",
    "    \n",
    "    def backward(self, input, dL_dy):\n",
    "        dL_dx, dL_df = conv_backward(input, dL_dy, self.weight, stride=self.stride,\\\n",
    "                                     padding=self.padding, dilation=self.dilation)\n",
    "        return dL_dx, dL_df\n",
    "    \n",
    "    \n",
    "class ConvTranspose2d():\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1):\n",
    "        self.in_channels  = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size  = kernel_size\n",
    "        self.stride   = stride\n",
    "        self.padding  = padding\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.weight   = torch.Tensor(in_channels, out_channels, kernel_size, kernel_size)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return conv_transpose2d(input, self.weight, stride=self.stride,\\\n",
    "                                padding=self.padding, dilation=self.dilation)\n",
    "    \n",
    "    __call__ = forward\n",
    "    \n",
    "    def backward(self, input, dL_dy):\n",
    "        p = self.kernel_size-1-self.padding\n",
    "        z = self.stride-1\n",
    "        \n",
    "        eff_input  = augment(input, nzeros=z, padding=p)\n",
    "        eff_weight = self.weight.flip(2,3).transpose(0,1)\n",
    "        dL_dx, dL_df = conv_backward(eff_input, dL_dy, eff_weight, stride=1, padding=0, dilation=1)\n",
    "        \n",
    "        dL_df = dL_df.flip(2,3).transpose(0,1)\n",
    "        return dL_dx[:,:,p:-p:z+1, p:-p:z+1], dL_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "918e1a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(x,y,decimals=7):\n",
    "    return torch.all(torch.round(torch.abs(x - y), decimals=decimals)==0.).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02bb054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4df1f302",
   "metadata": {},
   "source": [
    "## Test Forward Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "137cf7fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32, 32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = torch.load('../train_data.pkl')\n",
    "a = images[0][:1].float()\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e44439b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 3, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = torch.empty(5,3,3,3)\n",
    "kernel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79b7f89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kernel[0, 0] = torch.tensor([ [ +0., +0., -1. ], [ +0., +1., +0. ], [ -1., +0., +0. ]])\n",
    "kernel[1, 0] = torch.tensor([ [ +1., +1., +1. ], [ +1., +1., +1. ], [ +1., +1., +1. ]])\n",
    "kernel[2, 0] = torch.tensor([ [ -1., +0., +1. ], [ -1., +0., +1. ], [ -1., +0., +1. ]])\n",
    "kernel[3, 0] = torch.tensor([ [ -1., -1., -1. ], [ +0., +0., +0. ], [ +1., +1., +1. ]])\n",
    "kernel[4, 0] = torch.tensor([ [ +0., -1., +0. ], [ -1., +4., -1. ], [ +0., -1., +0. ]])\n",
    "\n",
    "for j in range(0,5):\n",
    "    for i in range(1,3):\n",
    "        kernel[j,i] = kernel[j,0]\n",
    "        \n",
    "        \n",
    "plt.imshow(kernel[0,0],cmap='seismic',vmin=-1,vmax=1)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c3ce9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 30, 30])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convolution with torch.functionals\n",
    "conv_a = F.conv2d(a, kernel, stride=1, padding=0, dilation=1)\n",
    "conv_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4ce0260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 30, 30])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Implemented convolution\n",
    "myconv = Conv2d(3,5, 3, stride=1, padding=0, dilation=1)\n",
    "myconv.weight = kernel\n",
    "myconv_a = myconv(a)\n",
    "myconv_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13a265e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify\n",
    "torch.all(conv_a == myconv_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7488c41c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31a127d4",
   "metadata": {},
   "source": [
    "## Test derivative MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15d4f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dL(y,trg):\n",
    "    return 2 * (y-trg) / y.shape.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "597419a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 32, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_input, valid_target = torch.load('../val_data.pkl')\n",
    "\n",
    "select=10\n",
    "y     = (valid_input[:select].double()/255.).requires_grad_()\n",
    "ytrue = (valid_target[:select].double()/255.).requires_grad_()\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c6e1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = F.mse_loss(y, ytrue)\n",
    "dL_dy = torch.autograd.grad(L, (y))[0]\n",
    "\n",
    "mydL_dy = dL(y,ytrue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35b93028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(dL_dy, mydL_dy, decimals=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a6d184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae211a83",
   "metadata": {},
   "source": [
    "## Test backward Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c0dfccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_input, valid_target = torch.load('../val_data.pkl')\n",
    "\n",
    "select=10\n",
    "x     = (valid_input[:select].float()).requires_grad_()\n",
    "xtrue = (valid_target[:select].float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1edaa964",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.empty(5,3,3,3)\n",
    "\n",
    "f[0, 0] = torch.tensor([ [ +0., +0., -1. ], [ +0., +1., +0. ], [ -1., +0., +0. ]])\n",
    "f[1, 0] = torch.tensor([ [ +1., +1., +1. ], [ +1., +1., +1. ], [ +1., +1., +1. ]])\n",
    "f[2, 0] = torch.tensor([ [ -1., +0., +1. ], [ -1., +0., +1. ], [ -1., +0., +1. ]])\n",
    "f[3, 0] = torch.tensor([ [ -1., -1., -1. ], [ +0., +0., +0. ], [ +1., +1., +1. ]])\n",
    "f[4, 0] = torch.tensor([ [ +0., -1., +0. ], [ -1., +4., -1. ], [ +0., -1., +0. ]])\n",
    "\n",
    "for j in range(0,5):\n",
    "    for i in range(1,3):\n",
    "        f[j,i] = f[j,0]\n",
    "        \n",
    "f.requires_grad_();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d6c0c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      "x : \t (10, 3, 32, 32)\n",
      "y : \t (10, 5, 15, 15)\n",
      "f : \t (5, 3, 3, 3)\n",
      "dL_dy :  (10, 5, 15, 15)\n",
      "dL_dx :  (10, 3, 32, 32)\n",
      "dL_df :  (5, 3, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "stride = 2\n",
    "y = F.conv2d(x, f, stride=stride)\n",
    "\n",
    "with torch.no_grad():\n",
    "    ytrue = F.conv2d(xtrue, f, stride=stride)\n",
    "\n",
    "L = F.mse_loss(y,ytrue)\n",
    "dL_dy, dL_dx, dL_df = torch.autograd.grad(L, (y,x,f))\n",
    "\n",
    "print(\"Shapes\")\n",
    "print(\"x : \\t\", tuple(x.shape))\n",
    "print(\"y : \\t\", tuple(y.shape))\n",
    "print(\"f : \\t\", tuple(f.shape))\n",
    "print(\"dL_dy : \", tuple(dL_dy.shape))\n",
    "print(\"dL_dx : \", tuple(dL_dx.shape))\n",
    "print(\"dL_df : \", tuple(dL_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39b5de76",
   "metadata": {},
   "outputs": [],
   "source": [
    "myconv = Conv2d(3 ,5, 3, stride=stride, padding=0, dilation=1)\n",
    "myconv.weight = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed976f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      "mydL_dx :  torch.Size([10, 3, 32, 32])\n",
      "mydL_df :  torch.Size([5, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "mydL_dx, mydL_df = myconv.backward(x, dL_dy)\n",
    "\n",
    "print(\"Shapes\")\n",
    "print(\"mydL_dx : \",mydL_dx.shape)\n",
    "print(\"mydL_df : \",mydL_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db7fea0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(dL_dx, mydL_dx, decimals=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "006dd24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(dL_df, mydL_df, decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32753a81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bef9e2f",
   "metadata": {},
   "source": [
    "## Test transposed convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79255d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd   = F.conv_transpose2d(x, f.transpose(0,1), stride=2, padding=0)\n",
    "mydd =   conv_transpose2d(x, f.transpose(0,1), stride=2, padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae8e8a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(dd,mydd,decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b71000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74c084c5",
   "metadata": {},
   "source": [
    "## Test backward Transposed Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d625388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_input, valid_target = torch.load('../val_data.pkl')\n",
    "\n",
    "select=10\n",
    "x     = (valid_input[:select].float()).requires_grad_()\n",
    "xtrue = (valid_target[:select].float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a0c0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.empty(5,3,3,3)\n",
    "\n",
    "f[0, 0] = torch.tensor([ [ +0., +0., -1. ], [ +0., +1., +0. ], [ -1., +0., +0. ]])\n",
    "f[1, 0] = torch.tensor([ [ +1., +1., +1. ], [ +1., +1., +1. ], [ +1., +1., +1. ]])\n",
    "f[2, 0] = torch.tensor([ [ -1., +0., +1. ], [ -1., +0., +1. ], [ -1., +0., +1. ]])\n",
    "f[3, 0] = torch.tensor([ [ -1., -1., -1. ], [ +0., +0., +0. ], [ +1., +1., +1. ]])\n",
    "f[4, 0] = torch.tensor([ [ +0., -1., +0. ], [ -1., +4., -1. ], [ +0., -1., +0. ]])\n",
    "\n",
    "for j in range(0,5):\n",
    "    for i in range(1,3):\n",
    "        f[j,i] = f[j,0]\n",
    "\n",
    "ff = f.transpose(0,1)\n",
    "ff.requires_grad_();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f077c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      "x : \t (10, 3, 32, 32)\n",
      "y : \t (10, 5, 65, 65)\n",
      "f : \t (5, 3, 3, 3)\n",
      "dL_dy :  (10, 5, 65, 65)\n",
      "dL_dx :  (10, 3, 32, 32)\n",
      "dL_df :  (3, 5, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "stride = 2\n",
    "y = F.conv_transpose2d(x, ff, stride=stride)\n",
    "\n",
    "with torch.no_grad():\n",
    "    ytrue = F.conv_transpose2d(xtrue, ff, stride=stride)\n",
    "\n",
    "L = F.mse_loss(y,ytrue)\n",
    "dL_dy, dL_dx, dL_df = torch.autograd.grad(L, (y,x,ff))\n",
    "\n",
    "print(\"Shapes\")\n",
    "print(\"x : \\t\", tuple(x.shape))\n",
    "print(\"y : \\t\", tuple(y.shape))\n",
    "print(\"f : \\t\", tuple(f.shape))\n",
    "print(\"dL_dy : \", tuple(dL_dy.shape))\n",
    "print(\"dL_dx : \", tuple(dL_dx.shape))\n",
    "print(\"dL_df : \", tuple(dL_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbdbef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7f696e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytconv = ConvTranspose2d(3 ,5, 3, stride=stride, padding=0, dilation=1)\n",
    "mytconv.weight = ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "55fc0a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      "mydL_dx :  torch.Size([10, 3, 32, 32])\n",
      "mydL_df :  torch.Size([3, 5, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "mydL_dx, mydL_df = mytconv.backward(x, dL_dy)\n",
    "\n",
    "print(\"Shapes\")\n",
    "print(\"mydL_dx : \",mydL_dx.shape)\n",
    "print(\"mydL_df : \",mydL_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5dcaaf06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(dL_dx, mydL_dx, decimals=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0022c508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(dL_df, mydL_df, decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a755107c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b4b3b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "87160787",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential():\n",
    "    def __init__(self, **kwargs):\n",
    "        self._modules: Dict[str, Optional['Module']] = OrderedDict()\n",
    "\n",
    "        for key, val in kwargs.items():\n",
    "            self.add_module(key, val)\n",
    "            \n",
    "    def add_module(self, name, module):\n",
    "        self._modules[name] = module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6a3b9fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('c', <__main__.ConvTranspose2d at 0x7efe5887f070>),\n",
       "             ('d', <__main__.Conv2d at 0x7efe5fdf56d0>)])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = Sequential(c=mytconv, d=myconv)\n",
    "\n",
    "s._modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0277813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1111f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f66f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97ee917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085ad66e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
